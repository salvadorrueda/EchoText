# EchoText Technical Documentation

Aquest document detalla l'arquitectura t√®cnica, el codi del servidor i la configuraci√≥ de Docker del projecte EchoText.

## üöÄ Tecnologia Utilitzada

El projecte es basa en les seg√ºents tecnologies clau:

- **Python 3.10**: Llenguatge principal de programaci√≥.
- **OpenAI Whisper (Model Turbo)**: Sistema de reconeixement de parla (ASR) d'estat de l'art.
- **Flask**: Framework web lleuger per servir l'API.
- **Waitress**: Servidor WSGI de producci√≥ per a Flask.
- **PyTorch**: Motor per a l'execuci√≥ del model Whisper (amb suport CUDA/CPU).
- **Docker**: Plataforma de contenidors per al desplegament a√Øllat.
- **FFmpeg**: Eina externa indispensable per al processament d'√†udio.

## üñ•Ô∏è Servidor de l'API (`api_server.py`)

El servidor actua com a pont entre el model Whisper i els clients (web o scripts).

### C√†rrega del Model
El servidor utilitza un fil paral¬∑lel o una funci√≥ d'inicialitzaci√≥ `load_model()` que intenta carregar la versi√≥ **"turbo"** de Whisper. 
- Si detecta una GPU NVIDIA (CUDA), l'utilitza autom√†ticament.
- Si no hi ha prou mem√≤ria (OOM), intenta carregar el model **"small"** com a alternativa.

### Rutes de l'API

1.  **`/` (GET)**: 
    - Serveix una p√†gina HTML generada a partir del fitxer `README.md`.
    - Utilitza la llibreria `markdown` de Python per convertir el format MD a HTML.
    - Inclou estils CSS incrustats per a una presentaci√≥ neta i llegible.

2.  **`/transcribe` (POST)**:
    - Rep un fitxer d'√†udio a trav√©s d'un formulari `multipart/form-data`.
    - Guarda temporalment el fitxer per processar-lo amb Whisper.
    - Suporta la detecci√≥ autom√†tica d'idioma o un par√†metre `language` opcional.
    - Retorna un JSON amb el text transcrit.

## üê≥ Docker i Desplegament

La imatge Docker permet desplegar el servidor sense instal¬∑lar depend√®ncies a l'host.

### `Dockerfile`
- **Base**: `python:3.10-slim` per mantenir la imatge petita.
- **Depend√®ncies del Sistema**: Instal¬∑la `ffmpeg`.
- **Requirements**: Instal¬∑la autom√†ticament totes les llibreries (`flask`, `openai-whisper`, `markdown`, etc.).
- **Codis**: Copia el codi de l'aplicaci√≥ (`api_server.py`), la llibreria personalitzada (`lib/`) i el `README.md`.

### Persist√®ncia del Model
Un dels reptes de Whisper en Docker √©s la mida del model (1.5 GB). Per evitar descarregar-lo en cada inici:
- S'utilitza un **Docker Volume** (`whisper-models`) muntat a `/root/.cache/whisper`.
- L'script `start_docker_server.sh` automatitza aquesta creaci√≥ i muntatge.

## üõ†Ô∏è Opcions i Configuraci√≥

- **CORS**: El servidor est√† configurat per permetre peticions des de qualsevol origen (`*`), fet necessari per a integracions web directes.
- **Multi-threading**: Flask s'executa darrere de **Waitress**, que gestiona m√∫ltiples peticions de forma eficient.
- **Error Handling**: El servidor retorna errors 500 o 503 si el model encara s'est√† carregant o ha fallat.


Per permetre l'acc√©s extern al servidor:

cloudflared tunnel --url http://localhost:5000

https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/do-more-with-tunnels/trycloudflare/ 
